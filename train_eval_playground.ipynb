{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfe2ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apoor\\mlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['battery', 'bulb', 'resistor', 'capacitor', 'inductor', 'transistor', 'ground', 'switch', 'diode']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([10, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 6.2198\n",
      "Epoch 2/25 - Loss: 5.5641\n",
      "Epoch 3/25 - Loss: 6.1578\n",
      "Epoch 4/25 - Loss: 5.8315\n",
      "Epoch 5/25 - Loss: 6.1796\n",
      "Epoch 6/25 - Loss: 5.8801\n",
      "Epoch 7/25 - Loss: 5.7165\n",
      "Epoch 8/25 - Loss: 5.3341\n",
      "Epoch 9/25 - Loss: 4.9680\n",
      "Epoch 10/25 - Loss: 4.7325\n",
      "Epoch 11/25 - Loss: 4.6846\n",
      "Epoch 12/25 - Loss: 4.7323\n",
      "Epoch 13/25 - Loss: 4.6897\n",
      "Epoch 14/25 - Loss: 4.5681\n",
      "Epoch 15/25 - Loss: 4.2957\n",
      "Epoch 16/25 - Loss: 4.3178\n",
      "Epoch 17/25 - Loss: 4.4523\n",
      "Epoch 18/25 - Loss: 4.2561\n",
      "Epoch 19/25 - Loss: 4.4949\n",
      "Epoch 20/25 - Loss: 4.3793\n",
      "Epoch 21/25 - Loss: 4.3040\n",
      "Epoch 22/25 - Loss: 4.3595\n",
      "Epoch 23/25 - Loss: 4.1932\n",
      "Epoch 24/25 - Loss: 4.3070\n",
      "Epoch 25/25 - Loss: 4.3742\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------- Load classes.txt ----------------------\n",
    "class_id_to_name = {}\n",
    "with open('dataset/classes.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            idx, name = line.split(':')\n",
    "            class_id_to_name[int(idx)] = name\n",
    "\n",
    "num_classes = len(class_id_to_name)\n",
    "class_names = [class_id_to_name[i] for i in range(num_classes)]\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# ---------------------- Custom Dataset ------------------------\n",
    "class YOLOCircuitDataset(Dataset):\n",
    "    def __init__(self, img_dir, anno_dir, processor):\n",
    "        self.img_dir = img_dir\n",
    "        self.anno_dir = anno_dir\n",
    "        self.processor = processor\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.lower().endswith('.jpg')]\n",
    "        self.img_files.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        # Parse YOLO txt annotations and wrap in COCO format\n",
    "        txt_name = os.path.splitext(img_file)[0] + \".txt\"\n",
    "        txt_path = os.path.join(self.anno_dir, txt_name)\n",
    "        annotations = []\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x_c, y_c, w, h = map(float, line.strip().split())\n",
    "                    x_c, y_c, w, h = x_c * width, y_c * height, w * width, h * height\n",
    "                    x_min = x_c - w / 2\n",
    "                    y_min = y_c - h / 2\n",
    "                    annotations.append({\n",
    "                        \"category_id\": int(class_id),\n",
    "                        \"bbox\": [x_min, y_min, w, h],   # COCO = [x_min, y_min, width, height]\n",
    "                        \"area\": w * h,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "\n",
    "        # COCO-style dict for the HF processor\n",
    "        target = {\n",
    "            \"image_id\": idx,\n",
    "            \"annotations\": annotations\n",
    "        }\n",
    "        encoding = self.processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        # Don't squeeze here: let collate_fn stack properly!\n",
    "        return encoding\n",
    "\n",
    "# -------------------- Custom collate_fn ----------------------\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batches HF encodings from YOLOCircuitDataset.\n",
    "    Each batch[i] is a dict of tensors (with leading batch dim 1).\n",
    "    \"\"\"\n",
    "    batched = {}\n",
    "    for k in batch[0]:\n",
    "        # pixel_values: [1,3,H,W] per item, so stack then squeeze\n",
    "        if k == \"pixel_values\":\n",
    "            batched[k] = torch.cat([item[k] for item in batch], dim=0)\n",
    "        # labels: each is a dict (no batch dim), so keep as list\n",
    "        elif k == \"labels\":\n",
    "            batched[k] = [item[k][0] for item in batch]  # [0] to drop singleton batch dim\n",
    "        else:\n",
    "            batched[k] = [item[k] for item in batch]\n",
    "    return batched\n",
    "\n",
    "# --------------------- Dataset and DataLoader ------------------\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "dataset = YOLOCircuitDataset(\"dataset/train/processed\", \"dataset/train/annotations\", processor)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# --------------------- Model Setup -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\",\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 25\n",
    "num_training_steps = num_epochs * len(loader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# --------------------- Training Loop ---------------------------\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# --------------------- Save Model ------------------------------\n",
    "os.makedirs(\"ckt_detr_fewshot\", exist_ok=True)\n",
    "model.save_pretrained(\"ckt_detr_fewshot\")\n",
    "processor.save_pretrained(\"ckt_detr_fewshot\")\n",
    "with open(\"ckt_detr_fewshot/class_names.txt\", \"w\") as f:\n",
    "    for name in class_names:\n",
    "        f.write(name + \"\\n\")\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1e7938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_results\\pred_000.jpg\n",
      "Evaluation done! Visualized predictions in eval_results/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load model & processor\n",
    "model_dir = \"ckt_detr_fewshot\"\n",
    "model = DetrForObjectDetection.from_pretrained(model_dir).eval()\n",
    "processor = DetrImageProcessor.from_pretrained(model_dir)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load class names\n",
    "with open(os.path.join(model_dir, \"class_names.txt\")) as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Output dir for results\n",
    "os.makedirs(\"eval_results\", exist_ok=True)\n",
    "\n",
    "# Inference on each test image\n",
    "test_dir = \"dataset/test/processed\"\n",
    "image_files = [f for f in os.listdir(test_dir) if f.lower().endswith(\".jpg\")]\n",
    "\n",
    "for img_name in image_files:\n",
    "    img_path = os.path.join(test_dir, img_name)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    # Preprocess\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)  # (H, W)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, \n",
    "        target_sizes=target_sizes, \n",
    "        threshold=0.135  # confidence threshold, adjust if needed\n",
    "    )[0]\n",
    "\n",
    "    # Draw boxes\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        score = float(score)\n",
    "        label = int(label)\n",
    "        box = [float(x) for x in box.tolist()]\n",
    "        x0, y0, x1, y1 = box\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=3)\n",
    "        class_label = class_names[label] if label < len(class_names) else str(label)\n",
    "        draw.text((x0, y0-10), f\"{class_label} {score:.2f}\", fill=\"red\")\n",
    "    \n",
    "    # Save visualized image\n",
    "    save_path = os.path.join(\"eval_results\", f\"pred_{img_name}\")\n",
    "    image.save(save_path)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"Evaluation done! Visualized predictions in eval_results/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
